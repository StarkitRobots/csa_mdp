#include "rosban_csa_mdp/solvers/pml2.h"

#include "rosban_bbo/optimizer_factory.h"

#include "rosban_csa_mdp/core/fa_policy.h"
#include "rosban_csa_mdp/core/policy_factory.h"

#include "rosban_fa/constant_approximator.h"
#include "rosban_fa/fake_split.h"
#include "rosban_fa/function_approximator_factory.h"
#include "rosban_fa/linear_approximator.h"
#include "rosban_fa/linear_split.h"
#include "rosban_fa/orthogonal_split.h"
#include "rosban_random/tools.h"
#include "rhoban_utils/timing/time_stamp.h"

using namespace rosban_fa;
using rhoban_utils::TimeStamp;

namespace csa_mdp
{

//TODO: those functions should be placed elsewhere, maybe static

/// Build a linear function using the provided information
std::unique_ptr<FunctionApproximator>
buildFAFromLinearParams(const Eigen::VectorXd & raw_params,
                        const Eigen::VectorXd & space_center,
                        int input_dim,
                        int output_dim,
                        int action_id)
{
  Eigen::VectorXd full_params((input_dim+1) * (output_dim+1));
  for (int col = 0; col < input_dim+1; col++) {
    int start = col * (output_dim+1);
    full_params(start) = col == 0 ? action_id : 0;
    full_params.segment(start+1, output_dim) = raw_params.segment(col*output_dim, output_dim);
  }
  return std::unique_ptr<FunctionApproximator>(new LinearApproximator(input_dim, output_dim + 1,
                                                                      full_params, space_center));
}

Eigen::VectorXd computeStatesCenter(const std::vector<Eigen::VectorXd> & states)
{
  if (states.size() == 0) {
    throw std::logic_error("PML2::computeStatesCenter: empty vector");
  }
  Eigen::VectorXd sum = states[0];
  for (size_t i = 1; i < states.size(); i++) {
    sum = sum + states[i];
  }
  return sum / states.size();
}

Eigen::VectorXd computeStatesDeviation(const std::vector<Eigen::VectorXd> & states)
{
  Eigen::VectorXd center = computeStatesCenter(states);
  Eigen::VectorXd squared_errors = Eigen::VectorXd::Zero(center.rows());
  for (size_t i = 0; i < states.size(); i++) {
    squared_errors += (states[i] - center).cwiseAbs2();//Add squared error for current state
  }
  return (squared_errors / states.size()).cwiseSqrt();  
}

PML2::PML2()
  : training_evaluations(50),
    split_margin(0.05),
    evaluations_ratio(-1),
    age_basis(1.02),
    use_linear_splits(false)
{
}

PML2::~PML2() {}

int PML2::getNbEvaluationTrials() const {
  return nb_evaluation_trials;
}

int PML2::getTrainingEvaluations() const {
  return training_evaluations;
}

int PML2::getOptimizerMaxCall() const {
  int evaluations_allowed = evaluations_ratio * getNbEvaluationTrials();
  return (int)(evaluations_allowed / getTrainingEvaluations());
}


void PML2::init(std::default_random_engine * engine) {
  // If a policy has been specified, try to extract a FATree from policy
  if (policy) {
    policy_tree = policy->extractFATree();
    policy_tree->updateNodesCount();
    policy = buildPolicy(*policy_tree);
    std::vector<int> leaves_id = policy_tree->getLeavesId();
    for (int leaf_id : leaves_id) {
      mutation_candidates[leaf_id].mutation_score = 1.0;
      mutation_candidates[leaf_id].last_training = 0;
    }
  }
  // If no policy were specified, start a new one with a fake split at the root
  else {
    std::unique_ptr<Split> split(new FakeSplit());
    // Default action choice is 0
    int action_id = 0;
    const Eigen::MatrixXd & action_limits = problem->getActionLimits(action_id);
    // Default action is the middle of the space
    Eigen::VectorXd action(action_limits.rows() + 1);
    action(0) = action_id;
    action.segment(1, action_limits.rows()) = (action_limits.col(0) + action_limits.col(1)) / 2;
    std::unique_ptr<FunctionApproximator> default_fa(new ConstantApproximator(action));
    std::vector<std::unique_ptr<FunctionApproximator>> fas;
    fas.push_back(std::move(default_fa));
    policy_tree.reset(new FATree(std::move(split), fas));
    policy = buildPolicy(*policy_tree);
    // Getting leaf_id formally
    policy_tree->updateNodesCount();
    Eigen::MatrixXd space_limits = problem->getStateLimits();
    Eigen::VectorXd space_center = (space_limits.col(0) + space_limits.col(1)) / 2;
    int leaf_id = policy_tree->getLeafId(space_center);
    // Adding mutation candidate
    MutationCandidate candidate;
    candidate.mutation_score = 1.0;
    candidate.last_training = 0;
    mutation_candidates[leaf_id] = (candidate);
  }
  policy_tree->save("policy_tree.bin");
  

  double avg_reward = evalAndGetStates(engine);
  updateMutationsScores();
  std::cout << "Initial Reward: " << avg_reward << std::endl;
}

void PML2::update(std::default_random_engine * engine) {
  TimeStamp start = TimeStamp::now();
  int mutation_id = getMutationId(engine);
  mutate(mutation_id, engine);
  TimeStamp post_mutation = TimeStamp::now();
  double new_reward = evalAndGetStates(engine);
  TimeStamp post_evaluation = TimeStamp::now();
  std::cout << "New reward: " << new_reward << std::endl;
  policy_tree->save("policy_tree.bin");
  updateMutationsScores();
  TimeStamp post_misc = TimeStamp::now();
  // Writing data
  writeTime("mutation"  , diffSec(start          , post_mutation  ));
  writeTime("evaluation", diffSec(post_mutation  , post_evaluation));
  writeTime("misc"      , diffSec(post_evaluation, post_misc      ));
  writeScore(new_reward);
}

Eigen::VectorXd PML2::optimize(rosban_bbo::Optimizer::RewardFunc rf,
                               const Eigen::MatrixXd & space,
                               const Eigen::VectorXd & guess,
                               std::default_random_engine * engine,
                               double evaluation_mult) {
  // If activated, set the maximal number of calls to the reward function to optimizer
  if (evaluations_ratio > 0) {
    optimizer->setMaxCalls(getOptimizerMaxCall() * evaluation_mult);
  }
  optimizer->setLimits(space);
  return optimizer->train(rf, guess, engine);
}


int PML2::getMutationId(std::default_random_engine * engine) {
  // Getting total_score
  double total_score = 0;
  double highest_score = std::numeric_limits<double>::lowest();
  for (const auto & e : mutation_candidates) {
    total_score += e.second.mutation_score;
    if (e.second.mutation_score > highest_score) {
      highest_score = e.second.mutation_score;
    }
  }
  // Getting corresponding element
  double c_score = std::uniform_real_distribution<double>(0, total_score)(*engine);

  std::cout << "getMutationId: (max,total) = (" << highest_score << ", "
            << total_score << ")" << std::endl;

  double acc = 0;
  for (const auto & e : mutation_candidates) {
    acc += e.second.mutation_score;
    if (acc > c_score) {
      std::cout << "<- Mutation candidate: '" << e.first << "' score: "
                << e.second.mutation_score << std::endl;
      return e.first;
    }
  }
  // Should never happen except with numerical errors
  std::ostringstream oss;
  oss << "PML2::getMutationId: c_score >= total_score"
      << " (" << c_score << ">=" << total_score << ")";
  throw std::logic_error(oss.str());
}

void PML2::setNbThreads(int nb_threads) {
  BlackBoxLearner::setNbThreads(nb_threads);
}

double PML2::evalAndGetStates(std::default_random_engine * engine)
{
  std::vector<Eigen::VectorXd> global_visited_states;
  double reward = evaluatePolicy(*policy, getNbEvaluationTrials(), engine,
                                 &global_visited_states);
  // Clear all visited states between two iterations
  for (auto & e : mutation_candidates) {
    e.second.visited_states.clear();
  }
  for (const Eigen::VectorXd & state : global_visited_states) {
    int node_idx = policy_tree->getLeafId(state);
    // node_idx is expected to be present in mutation_candidates
    mutation_candidates.at(node_idx).visited_states.push_back(state);
  }
  return reward;
}

void PML2::updateMutationsScores() {
  for (auto & e : mutation_candidates) {
    MutationCandidate & c = e.second;
    // Computing age score
    double age = iterations - c.last_training;
    double relative_age = age / mutation_candidates.size();
    double age_score = pow(age_basis, relative_age);
    // Computing density score
    int nb_states = c.visited_states.size();
    double density_score = 1 + nb_states;// avoid 0 score
    // Updating mutation score
    c.mutation_score = age_score * density_score;
  }
}

void PML2::mutate(int mutation_id,
                  std::default_random_engine * engine) {
  if (isMutationAllowed(mutation_candidates[mutation_id])) {
    mutateLeaf(mutation_id, engine);
  }
  else {
    std::cout << "-> Skipping mutation (forbidden)" << std::endl;
    mutation_candidates[mutation_id].last_training = iterations;
  }
}

void PML2::mutateLeaf(int mutation_id,
                      std::default_random_engine * engine) {

  for (int action_id = 0; action_id < problem->getNbActions(); action_id++) {
    tryRefine(mutation_id, action_id, engine);
  }
  if (use_linear_splits) {
    applyBestLinearSplit(mutation_id, engine);
  } else { 
    applyBestSplit(mutation_id, engine);
  }
}

void PML2::tryRefine(int mutation_id, int action_id,
                     std::default_random_engine * engine) {
  // Get reference to the appropriate mutation
  MutationCandidate * mutation = &(mutation_candidates[mutation_id]);
  // Initial states
  std::vector<Eigen::VectorXd> initial_states = getInitialStates(*mutation, engine);
  std::cout << "-> Nb Initial states: " << initial_states.size() << std::endl;
  // Get space, and space center
  int input_dim = problem->stateDims();
  Eigen::VectorXd samples_center = computeStatesCenter(initial_states);
  // Test action of type
  std::cout << "Training for action to " << action_id << std::endl;
  int output_dim = problem->actionDims(action_id);
  Eigen::MatrixXd action_limits = problem->getActionLimits(action_id);
  // Training function
  rosban_bbo::Optimizer::RewardFunc reward_func =
    [this, input_dim, output_dim, &samples_center, &initial_states,
     action_id]
    (const Eigen::VectorXd & parameters,
     std::default_random_engine * engine)
    {
      // Build the tree
      std::unique_ptr<FunctionApproximator> new_approximator;
      new_approximator = buildFAFromLinearParams(parameters,
                                                 samples_center,
                                                 input_dim,
                                                 output_dim,
                                                 action_id);
      std::unique_ptr<FATree> new_tree;
      new_tree = policy_tree->copyAndReplaceLeaf(initial_states[0], std::move(new_approximator));
      //TODO: avoid this second copy of the tree which is not necessary
      std::unique_ptr<Policy> policy = buildPolicy(*new_tree);
      // Type of evaluation depends on the fact that initial_states have been created
      return evaluation(*policy, initial_states, engine);
    };
  // Getting parameters_space
  Eigen::MatrixXd parameters_space = getParametersSpaces(action_id);
  Eigen::VectorXd parameters_guess = Eigen::VectorXd::Zero(parameters_space.rows());
  for (int dim = 0; dim < parameters_guess.rows(); dim++) {
    double original = parameters_guess(dim);
    double min = parameters_space(dim, 0);
    double max = parameters_space(dim, 1);
    parameters_guess(dim) = std::min(max,std::max(min,original));
  }

  // Optimization phase
  Eigen::VectorXd refined_parameters = optimize(reward_func,
                                                parameters_space,
                                                parameters_guess,
                                                engine);

  // Creating refined approximator
  std::unique_ptr<FunctionApproximator> refined_approximator;
  refined_approximator = buildFAFromLinearParams(refined_parameters,
                                                 samples_center,
                                                 input_dim, output_dim, action_id);
  // Creating refined policy
  std::unique_ptr<FATree> refined_tree;
  refined_tree = policy_tree->copyAndReplaceLeaf(initial_states[0], std::move(refined_approximator));
  // Submit the new tree
  submitTree(std::move(refined_tree), initial_states, engine);
  // Update mutation properties:
  mutation->last_training = iterations;
}

void PML2::applyBestSplit(int mutation_id,
                          std::default_random_engine * engine) {
  const MutationCandidate & mutation = mutation_candidates[mutation_id];
  std::vector<Eigen::VectorXd> initial_states = getInitialStates(mutation, engine);
  Eigen::VectorXd samples_center = computeStatesCenter(initial_states);
  const FunctionApproximator & leaf_fa = policy_tree->getLeafApproximator(initial_states[0]);
  // Debug message
  std::cout << "-> Applying a split mutation " << std::endl;
  std::cout << "-> Nb initial states: " << initial_states.size() << std::endl;
  // Testing all dimensions as split and keeping the best one
  double best_score = std::numeric_limits<double>::lowest();
  std::unique_ptr<FATree> best_tree;
  for (int dim = 0; dim < problem->stateDims(); dim++) {
    double score;
    std::unique_ptr<FATree> current_tree = trySplit(dim, initial_states,
                                                    engine, &score);
    if (score > best_score) {
      best_score = score;
      best_tree = std::move(current_tree);
    }
  }
  // Avoiding risk of segfault if something wrong happened before
  if (!best_tree) {
    throw std::logic_error("PML2::splitMutation: Failed to find a best tree");
  }
  // Getting ref to the chosen Split and backing it up in case it is needed after
  const Split & split = best_tree->getPreLeafApproximator(initial_states[0]).getSplit();
  std::unique_ptr<Split> split_copy = split.clone();
  // Evaluating policy with respect to previous solution
  int nb_new_nodes = split.getNbElements();
  bool replaced_policy = submitTree(std::move(best_tree), initial_states, engine);
  // If the new approximation does not replace current one, reuse current FA for children
  if (!replaced_policy) {
    // Create a new FATree with cloned approximators
    std::vector<std::unique_ptr<FunctionApproximator>> approximators;
    for (int elem = 0; elem < split_copy->getNbElements(); elem++) {
      approximators.push_back(leaf_fa.clone());
    }
    std::unique_ptr<FATree> new_leaf_fa(new FATree(std::move(split_copy),
                                                   approximators));
    // Replace it in policy tree and update policy
    policy_tree->replaceApproximator(initial_states[0], std::move(new_leaf_fa));
    policy = buildPolicy(*policy_tree);
  }
  postSplitUpdate(mutation_id, nb_new_nodes);
}



void PML2::applyBestLinearSplit(int mutation_id,
                                std::default_random_engine * engine) {
  const MutationCandidate & mutation = mutation_candidates[mutation_id];
  std::vector<Eigen::VectorXd> initial_states = getInitialStates(mutation, engine);
  Eigen::VectorXd samples_center = computeStatesCenter(initial_states);
  const FunctionApproximator & leaf_fa = policy_tree->getLeafApproximator(initial_states[0]);
  // Debug message
  std::cout << "-> Applying a linear split mutation " << std::endl;
  std::cout << "-> Nb initial states: " << initial_states.size() << std::endl;
  // Testing all actions as split and keeping the best one
  double best_score = std::numeric_limits<double>::lowest();
  std::unique_ptr<FATree> best_tree;
  for (int action_id = 0; action_id < problem->getNbActions(); action_id++) {
    double score;
    std::unique_ptr<FATree> current_tree = tryLinearSplit(action_id, initial_states,
                                                          engine, &score);
    if (score > best_score) {
      best_score = score;
      best_tree = std::move(current_tree);
    }
  }
  // Avoiding risk of segfault if something wrong happened before
  if (!best_tree) {
    throw std::logic_error("PML2::applyBestLinearSplit: Failed to find a best tree");
  }
  // TODO: this part is duplicated from applyBestOrthogonalSplit code shall be shared
  // Getting ref to the chosen Split and backing it up in case it is needed after
  const Split & split = best_tree->getPreLeafApproximator(initial_states[0]).getSplit();
  std::unique_ptr<Split> split_copy = split.clone();
  // Evaluating policy with respect to previous solution
  int nb_new_nodes = split.getNbElements();
  bool replaced_policy = submitTree(std::move(best_tree), initial_states, engine);
  // If the new approximation does not replace current one, reuse current FA for children
  if (!replaced_policy) {
    // Create a new FATree with cloned approximators
    std::vector<std::unique_ptr<FunctionApproximator>> approximators;
    for (int elem = 0; elem < split_copy->getNbElements(); elem++) {
      approximators.push_back(leaf_fa.clone());
    }
    std::unique_ptr<FATree> new_leaf_fa(new FATree(std::move(split_copy),
                                                   approximators));
    // Replace it in policy tree and update policy
    policy_tree->replaceApproximator(initial_states[0], std::move(new_leaf_fa));
    policy = buildPolicy(*policy_tree);
  }
  postSplitUpdate(mutation_id, nb_new_nodes);
}

std::unique_ptr<rosban_fa::FATree>
PML2::trySplit(int split_dim,
               const std::vector<Eigen::VectorXd> & initial_states,
               std::default_random_engine * engine,
               double * score) {
  // Debug message
  std::cout << "\tTesting split along " << split_dim << std::endl;
  // Getting center of samples
  Eigen::MatrixXd samples_center = computeStatesCenter(initial_states);
  // Getting current action_id
  int action_id = (int)policy_tree->predict(initial_states[0])(0);
  Eigen::MatrixXd action_limits = problem->getActionLimits(action_id);
  int action_dims = problem->actionDims(action_id);
  // Function to train has the following parameters:
  // - Position of the split
  // - Action in each part of the split
  auto parameters_to_approximator =
    [action_dims, split_dim, action_id]
    (const Eigen::VectorXd & parameters)
    {
      // Importing values from parameters
      double split_val = parameters[0];
      // Adding action_id
      Eigen::VectorXd params0 = Eigen::VectorXd::Zero(action_dims+1);
      params0(0) = action_id;
      params0.segment(1,action_dims) = parameters.segment(1, action_dims);
      Eigen::VectorXd params1 = Eigen::VectorXd::Zero(action_dims+1);
      params1(0) = action_id;
      params1.segment(1,action_dims) = parameters.segment(1 + action_dims, action_dims);
      // Building FunctionApproximator
      std::unique_ptr<FunctionApproximator> action0(new ConstantApproximator(params0));
      std::unique_ptr<FunctionApproximator> action1(new ConstantApproximator(params1));
      // Define Function approximator which will replace
      std::unique_ptr<Split> split(new OrthogonalSplit(split_dim, split_val));
      std::vector<std::unique_ptr<FunctionApproximator>> approximators;
      approximators.push_back(std::move(action0));
      approximators.push_back(std::move(action1));
      return std::unique_ptr<FATree>(new FATree(std::move(split), approximators));
    };
  // Defining evaluation function
  rosban_bbo::Optimizer::RewardFunc reward_func =
    [this, split_dim, action_dims, parameters_to_approximator,
     &initial_states]
    (const Eigen::VectorXd & parameters,
     std::default_random_engine * engine)
    {
      std::unique_ptr<FATree> new_approximator;
      new_approximator = parameters_to_approximator(parameters);
      // Replace approximator
      std::unique_ptr<FATree> new_tree;
      new_tree = policy_tree->copyAndReplaceLeaf(initial_states[0], std::move(new_approximator));
      // build policy and evaluate
      // TODO: avoid this second copy of the tree which is not necessary
      std::unique_ptr<Policy> policy = buildPolicy(*new_tree);
      return evaluation(*policy, initial_states, engine);
    };
  // Computing boundaries for split
  double dim_min = std::numeric_limits<double>::max();
  double dim_max = std::numeric_limits<double>::lowest();
  for (const Eigen::VectorXd & state : initial_states) {
    double v = state(split_dim);
    if (v < dim_min) dim_min = v;
    if (v > dim_max) dim_max = v;
  }
  double delta = (dim_max - dim_min) * split_margin;
  double split_min = dim_min + delta;
  double split_max = dim_max - delta;
  std::cout << "\t->Split value range:  [" << split_min << ", " << split_max << "]"
            << std::endl;
  // Define parameters_space
  Eigen::MatrixXd parameters_space(1+2*action_dims,2);
  parameters_space(0,0) = split_min;
  parameters_space(0,1) = split_max;
  parameters_space.block(1,0,action_dims,2) = action_limits;
  parameters_space.block(1+action_dims,0,action_dims,2) = action_limits;
  // Computing initial parameters using current approximator
  double split_default_val = (split_min + split_max) / 2;
  std::unique_ptr<Split> initial_split(new OrthogonalSplit(split_dim, split_default_val));
  Eigen::VectorXd initial_parameters(1+2*action_dims);
  Eigen::VectorXd default_action = policy_tree->predict(samples_center);
  initial_parameters(0) = split_default_val;
  for (int leaf_id = 0; leaf_id < initial_split->getNbElements(); leaf_id++) {
    int start = 1 + action_dims * leaf_id;
    initial_parameters.segment(start, action_dims) = default_action.segment(1,action_dims);
  }
  // Optimize
  Eigen::VectorXd optimized_parameters = optimize(reward_func,
                                                  parameters_space,
                                                  initial_parameters,
                                                  engine);
  // TODO: Evalually, evaluate with a larger set here  
  *score = reward_func(optimized_parameters, engine);
  // Debug
  std::cout << "\t->Optimized params: " << optimized_parameters.transpose() << std::endl;
  std::cout << "\t->Avg reward: " << *score << std::endl;
  // Build corresponding tree
  std::unique_ptr<FunctionApproximator> optimized_approximator;
  optimized_approximator = parameters_to_approximator(optimized_parameters);
  std::unique_ptr<FATree> splitted_tree;
  splitted_tree = policy_tree->copyAndReplaceLeaf(initial_states[0],
                                                  std::move(optimized_approximator));
  // return optimized approximator
  return std::move(splitted_tree);
}

/// Optimize a split wit the chosen action id
///
/// Function to optimize has the following parameters
/// 0                  : hyperplane offset factor in [-1 1] (with respect to samples center)
/// 1 to D             : hyperplane coeffs (with D input dimensionality) [-1 1] for each coeff
/// D+1 to (D+1)(A+1)  : action coefficients (cf getParametersSpace) ((D+1)*A coeffs)
/// with: D input dimension and A action space dimension
std::unique_ptr<rosban_fa::FATree>
PML2::tryLinearSplit(int action_id,
                     const std::vector<Eigen::VectorXd> & initial_states,
                     std::default_random_engine * engine,
                     double * score)
{
  // Debug message
  std::cout << "\tTesting linear split with action: " << action_id << std::endl;
  // Getting dimensions
  int D = problem->stateDims();
  int A = problem->actionDims(action_id);
  // Parameters space
  int param_dims = (A+1)*(D+1);
  Eigen::MatrixXd parameters_space(param_dims,2);
  for (int i = 0; i <= D; i++) {
    parameters_space(i,0) = -1;
    parameters_space(i,1) = 1;
  }
  parameters_space.block(D+1,0,(D+1)*A,2) = getParametersSpaces(action_id);
  // Initial guess
  // - Impossible to use a normal vector with only 0 -> arbitrary choice for initial vector
  // - Middle of the spaces for linear coefficients (actions are not necessarily centered in 0)
  Eigen::VectorXd initial_parameters = Eigen::VectorXd::Zero(param_dims);
  initial_parameters(1) = 1;
  initial_parameters.segment(D+1,(D+1)*A) =
    ( parameters_space.block(D+1,0,(D+1)*A,1) + parameters_space.block(D+1,1,(D+1)*A,1)) / 2;
  // Getting mutation properties
  Eigen::VectorXd samples_center = computeStatesCenter(initial_states);
  Eigen::VectorXd samples_dev = computeStatesDeviation(initial_states);
  const FunctionApproximator & leaf_fa = policy_tree->getLeafApproximator(initial_states[0]);
  auto parameters_to_approximator =
    [action_id,A,D,&samples_center,&samples_dev, &leaf_fa]
    (const Eigen::VectorXd & parameters)
    {
      // Importing values from parameters
      double offset_param = parameters(0);
      const Eigen::VectorXd & hyperplane_coeffs_param = parameters.segment(1,D);
      const Eigen::VectorXd & linear_approximator_params = parameters.segment(D+1,A*(D+1));
      // Extracting hyperplane
      // - Offset of plan is based on deviation of samples, hyperplane coeffs and params
      //   (by using absolute coeffs, we ensure that there is no possible compensation)
      Eigen::VectorXd hyperplane_coeffs = hyperplane_coeffs_param.normalized();
      double offset = -hyperplane_coeffs.dot(samples_center);
      offset += samples_dev.dot(hyperplane_coeffs.cwiseAbs()) * offset_param;
      // - Problematic because of the signs
      // Extracting new function approximator
      std::unique_ptr<FunctionApproximator> new_approximator;
      new_approximator = buildFAFromLinearParams(linear_approximator_params,
                                                 samples_center,
                                                 D, A, action_id);
      // Copying old function approximator
      std::unique_ptr<FunctionApproximator> old_approximator;
      old_approximator = leaf_fa.clone();
      // Define Function approximator which will replace
      std::unique_ptr<Split> split(new LinearSplit(hyperplane_coeffs, offset));
      std::vector<std::unique_ptr<FunctionApproximator>> approximators;
      approximators.push_back(std::move(new_approximator));
      approximators.push_back(std::move(old_approximator));
      return std::unique_ptr<FATree>(new FATree(std::move(split), approximators));
    };
  // Defining evaluation function
  rosban_bbo::Optimizer::RewardFunc reward_func =
    [this, samples_center, parameters_to_approximator,
     &initial_states]
    (const Eigen::VectorXd & parameters,
     std::default_random_engine * engine)
    {
      std::unique_ptr<FATree> new_approximator;
      new_approximator = parameters_to_approximator(parameters);
      // Replace approximator
      std::unique_ptr<FATree> new_tree;
      new_tree = policy_tree->copyAndReplaceLeaf(initial_states[0], std::move(new_approximator));
      // build policy and evaluate
      // TODO: avoid this second copy of the tree which is not necessary
      std::unique_ptr<Policy> policy = buildPolicy(*new_tree);
      return evaluation(*policy, initial_states, engine);
    };
  Eigen::VectorXd optimized_parameters = optimize(reward_func,
                                                  parameters_space,
                                                  initial_parameters,
                                                  engine);
  // TODO: Evaluate with a larger set
  *score = reward_func(optimized_parameters, engine);
  // Debug
  std::cout << "\t->Optimized params: " << optimized_parameters.transpose() << std::endl;
  std::cout << "\t->Avg reward: " << *score << std::endl;
  // return splitted tree
  std::unique_ptr<FunctionApproximator> optimized_approximator;
  optimized_approximator = parameters_to_approximator(optimized_parameters);
  std::unique_ptr<FATree> splitted_tree;
  splitted_tree = policy_tree->copyAndReplaceLeaf(initial_states[0],
                                                  std::move(optimized_approximator));
  return std::move(splitted_tree);
}


Eigen::MatrixXd PML2::getParametersSpaces(int action_id) const {
  // Which space is used for parameters
  Eigen::MatrixXd parameters_space;
  Eigen::MatrixXd space_for_parameters = problem->getStateLimits();
  parameters_space = LinearApproximator::getParametersSpace(space_for_parameters,
                                                            problem->getActionLimits(action_id),
                                                            false);
  return parameters_space;
}

std::string PML2::getClassName() const {
  return "PML2";
}

void PML2::to_xml(std::ostream &out) const {
  //TODO
  (void) out;
  throw std::logic_error("PML2::to_xml: not implemented");
}

void PML2::from_xml(TiXmlNode *node) {
  // Calling parent implementation
  BlackBoxLearner::from_xml(node);
  // Reading class variables
  rhoban_utils::xml_tools::try_read<int>   (node, "training_evaluations" , training_evaluations );
  rhoban_utils::xml_tools::try_read<double>(node, "split_probability"   , split_probability   );
  rhoban_utils::xml_tools::try_read<double>(node, "split_margin"        , split_margin        );
  rhoban_utils::xml_tools::try_read<double>(node, "evaluations_ratio"   , evaluations_ratio   );
  rhoban_utils::xml_tools::try_read<double>(node, "age_basis"           , age_basis           );
  rhoban_utils::xml_tools::try_read<bool>  (node, "use_linear_splits"   , use_linear_splits   );
  // Optimizer is mandatory
  optimizer = rosban_bbo::OptimizerFactory().read(node, "optimizer");
  // Read Policy if provided (optional)
  PolicyFactory().tryRead(node, "policy", policy);
  // Performing some checks
  if (split_margin < 0 || split_margin >= 0.5) {
    throw std::logic_error("PML2::from_xml: invalid value for split_margin");
  }
  //TODO: add checks on probability
  // Synchronize number of threads
  setNbThreads(nb_threads);
}

std::unique_ptr<Policy> PML2::buildPolicy(const FATree & tree) {
  std::unique_ptr<FATree> tree_copy(static_cast<FATree *>(tree.clone().release()));
  std::unique_ptr<Policy> result(new FAPolicy(std::move(tree_copy)));
  result->setActionLimits(problem->getActionsLimits());
  return std::move(result);
}


std::vector<Eigen::VectorXd>
PML2::getInitialStates(const MutationCandidate & mc,
                       std::default_random_engine * engine)
{
  size_t nb_evaluations_allowed = getTrainingEvaluations();
  // If we are lacking samples return them all
  if (nb_evaluations_allowed >= mc.visited_states.size()) {
    return mc.visited_states;
  }
  // Filter most important samples
  std::vector<Eigen::VectorXd> initial_states;
  std::vector<size_t> indices =
    rosban_random::getKDistinctFromN(nb_evaluations_allowed, 
                                     mc.visited_states.size(),
                                     engine);
  for (size_t idx : indices) {
    initial_states.push_back(mc.visited_states[idx]);
  }
  return initial_states;
}

bool PML2::isMutationAllowed(const MutationCandidate & mc) const
{
  return mc.visited_states.size() > 10;
}

bool PML2::submitTree(std::unique_ptr<rosban_fa::FATree> new_tree,
                      const std::vector<Eigen::VectorXd> & initial_states,
                      std::default_random_engine * engine)
{
  std::unique_ptr<Policy> new_policy = buildPolicy(*new_tree);
  // Evaluate old and new policy with updated parameters (on local space)
  double old_local_reward  = evaluation(*policy    , initial_states, engine);
  double new_local_reward  = evaluation(*new_policy, initial_states, engine);
  double old_global_reward = evaluatePolicy(*policy, getNbEvaluationTrials(), engine);
  double new_global_reward = evaluatePolicy(*new_policy, getNbEvaluationTrials(), engine);
  std::cout << "\told local reward: " << old_local_reward << std::endl;
  std::cout << "\tnew local reward: " << new_local_reward << std::endl;
  std::cout << "\told global reward: " << old_global_reward << std::endl;
  std::cout << "\tnew global reward: " << new_global_reward << std::endl;
  // Replace current if improvement has been seen both locally and globally
  if (new_local_reward > old_local_reward &&
      new_global_reward > old_global_reward) {
    policy_tree = std::move(new_tree);
    policy = std::move(new_policy);
    return true;
  }
  return false;
}

void PML2::postSplitUpdate(int node_id, int nb_nodes_added)
{
  int max_id_before = policy_tree->getNodesCount();
  // Moving existing mutation candidates
  for (int old_idx = max_id_before; old_idx > node_id; old_idx--) {
    if (mutation_candidates.count(old_idx) > 0) {
      mutation_candidates[old_idx+nb_nodes_added] = mutation_candidates[old_idx];
      mutation_candidates.erase(old_idx);
    }
  }
  // Adding new candidates
  for (int child = 1; child <= node_id + nb_nodes_added; child++) {
    mutation_candidates[node_id + child].mutation_score = 0;
    mutation_candidates[node_id + child].last_training = iterations;
  }
  // Removing splitted node mutation candidate
  mutation_candidates.erase(node_id);
  // Updating policy tree nodes count
  policy_tree->updateNodesCount();
}

}
